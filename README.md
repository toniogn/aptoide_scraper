# Aptoide Scraper

## Introduction

Aptoide Scraper is a demonstration web service dedicated to scrap some information about aptoide applications by just copy-pasting its url `https://app_to_scrap.en.aptoide.com/app` into an url field and clicking on the scrap button to let the magic happens.

> The application has been developped with django as web-service framework, requests for http requesting and beautiful soup for http response scraping with html5lib for html parsing.

## Code Base

The code is organized as following (the uncommented files are the ones automatically generated by django and not modified later):
```
-> .gitignore #git ignored files settings
-> README.md #you are here
-> requirements.txt #third party dependencies
-> aptoide_scraper #root folder
-----> .coveragerc #coverage settings file
-----> .mypy.ini #mypy settings file
-----> db.sqlite3 #django database
-----> manage.py #django running utilitary
-----> aptoide_scraper #django project folder
---------> asgi.py
---------> settings.py #settings of the django project
---------> urls.py #url configuration of the django project
---------> wsgi.py
-----> core: #django core application folder
---------> migrations #core application's models migrations folder
---------> templates #core application's views html templates
-------------> detail.html #html template of the detail view of a scraped aptoide app
-------------> index.html #html template of the index view of the core application
---------> tests: #core application's tests
-------------> test_unit.py #unit tests of the core application
-------------> test_functional.py #functional tests of the core application
---------> admin.py
---------> apps.py
---------> forms.py #core application's custom forms to use in FormView views
---------> models.py #core application's orm to map data to django's database
---------> urls.py #url configuration of the core application
---------> views.py #views of the core application
```

> Eached scraped application is interpreted as a django model instance and thus stored into database. We could imagine further treatments based on this storage like using stored data for multiple scrap requests to the same aptoide's application within a given time lapse.

## Behavior

The core application's behavior is controlled by its two views, the index view accessed at home page which displays a form with an aptoide application url field to fill. A submit button tries a form validation, once validated the aptoide application url is scraped, the corresponding model is created or updated in the database and the user is redirected to the detail view of the scraped application in case of success.

## Installation

First of all you should create a local clone of the code repository:
```powershell
git clone https://github.com/toniogn/aptoide_scraper.git
```

Before running the application first install dependencies in a python virtual environment with:
```powershell
python -m venv your_venv_name
your_venv_name/Scripts/activate
pip install -r requirements.txt
cd aptoide_scraper
```

## Type checking

To type-check the web serviceÂ´s code base you should run:
```powershell
mypy core/
mypy aptoide_scraper/
```
The .mypy.ini files specifies that errors coming from django, requests and bs4 third party dependencies should be ignored.

## Usage

Then you can either run the django server with:
```powershell
python manage.py runserver
```
> Visit the url http://127.0.0.1:8000/ to use the scraping web service.

Or you can measure the unit test coverage in the core application, the coverage configurations are detailed in the aptoide_scraper/.coveragerc file:
```powershell
coverage run manage.py test core.tests.test_unit
coverage report
```

Or even run functional test to ensure multilangage functionning on a given aptoide application's url panel:
```powershell
python manage.py test core.tests.test_functional
```

## Troubleshooting

Everything should work fine if you are not working behind a proxy, otherwise you could receive some implied connection errors. If any other kind of error occurs, don't hesistate to open an issue, it would be fixed as soon as possible.
